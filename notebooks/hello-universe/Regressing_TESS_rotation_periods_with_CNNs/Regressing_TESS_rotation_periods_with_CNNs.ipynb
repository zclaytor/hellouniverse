{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4479a40",
   "metadata": {},
   "source": [
    "# <span style='background:blue'>  <span style=\"color:orange\"> ***Reviewer Note:*** **Begin PEP8 check cells (delete below when finished)**</span> </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14322856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable all imported packages' loggers\n",
    "import logging\n",
    "logging.root.manager.loggerDict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c390fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on --ignore E261,E501,W291,W293\n",
    "\n",
    "# only allow the checker to throw warnings when there's a violation\n",
    "logging.getLogger('flake8').setLevel('ERROR')\n",
    "logging.getLogger('stpipe').setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0105e4",
   "metadata": {},
   "source": [
    "# <span style='background:blue'>  <span style=\"color:orange\"> ***Reviewer Note:*** **End PEP8 check cells (delete above when finished)**</span> </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e8db64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/zclaytor/smartsnn\n",
      "  Cloning https://github.com/zclaytor/smartsnn to /private/var/folders/04/6pd9ml351599_h45qtqtg_7m0001md/T/pip-req-build-zht30vtf\n",
      "  Running command git clone -q https://github.com/zclaytor/smartsnn /private/var/folders/04/6pd9ml351599_h45qtqtg_7m0001md/T/pip-req-build-zht30vtf\n",
      "Requirement already satisfied: numpy in /Users/cmurray1/anaconda3/lib/python3.8/site-packages (from smartsnn==0.0.0.dev1) (1.23.0)\n",
      "Requirement already satisfied: torch in /Users/cmurray1/anaconda3/lib/python3.8/site-packages (from smartsnn==0.0.0.dev1) (1.12.0)\n",
      "Requirement already satisfied: astropy in /Users/cmurray1/anaconda3/lib/python3.8/site-packages (from smartsnn==0.0.0.dev1) (5.0.2)\n",
      "Requirement already satisfied: packaging>=19.0 in /Users/cmurray1/anaconda3/lib/python3.8/site-packages (from astropy->smartsnn==0.0.0.dev1) (20.9)\n",
      "Requirement already satisfied: pyerfa>=2.0 in /Users/cmurray1/anaconda3/lib/python3.8/site-packages (from astropy->smartsnn==0.0.0.dev1) (2.0.0.1)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /Users/cmurray1/anaconda3/lib/python3.8/site-packages (from astropy->smartsnn==0.0.0.dev1) (6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/cmurray1/anaconda3/lib/python3.8/site-packages (from packaging>=19.0->astropy->smartsnn==0.0.0.dev1) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions in /Users/cmurray1/anaconda3/lib/python3.8/site-packages (from torch->smartsnn==0.0.0.dev1) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/zclaytor/smartsnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71cc70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from smartsnn.model import ConvNet\n",
    "from smartsnn.model import Laplacian_NLL as loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a002abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:1: E302 expected 2 blank lines, found 1\n",
      "6:1: E302 expected 2 blank lines, found 1\n",
      "9:1: E302 expected 2 blank lines, found 1\n"
     ]
    }
   ],
   "source": [
    "pmax = 180\n",
    "\n",
    "def scale_data(x):\n",
    "    return x/pmax\n",
    "\n",
    "def unscale_data(y):\n",
    "    return y*pmax\n",
    "\n",
    "class WaveletDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path, mode):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mode (string): either train, val, or test\n",
    "        \"\"\"\n",
    "        # need to handle mode; separate train/valid/test data\n",
    "        if mode == \"train\":\n",
    "            filenames = sorted(glob(data_path + \"/002/*fits\"))[0:3000:3] # How to read \"all\" data?\n",
    "        elif mode == \"val\":\n",
    "            filenames = sorted(glob(data_path + \"/002/*fits\"))[1:3001:3]\n",
    "        elif mode == \"test\":\n",
    "            filenames = sorted(glob(data_path + \"/002/*fits\"))[2:3002:3]\n",
    "\n",
    "        periods = []\n",
    "        wavelets = []\n",
    "        \n",
    "        for filename in filenames: # How to read \"all\" data?\n",
    "            f = fits.open(filename)\n",
    "            periods.append(f[0].header[\"PERIOD\"])\n",
    "            wavelets.append(f[2].data)\n",
    "            f.close()\n",
    "        self.wavelets = np.stack(wavelets)\n",
    "        self.periods = np.array(periods)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.periods)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        X = self.wavelets[idx].astype('float32') \n",
    "        X = torch.tensor(X/255)\n",
    "        X = torch.unsqueeze(X, 0)\n",
    "        label = torch.tensor(self.periods[idx, np.newaxis])\n",
    "        return X, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "034d9728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:9: E265 block comment should start with '# '\n",
      "41:1: E302 expected 2 blank lines, found 1\n",
      "63:1: E302 expected 2 blank lines, found 1\n",
      "82:1: E302 expected 2 blank lines, found 1\n",
      "97:13: E265 block comment should start with '# '\n",
      "104:9: E303 too many blank lines (2)\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, val_loader, num_epochs=100, early_stopping_patience=10, device=torch.device(\"cpu\")):\n",
    "    '''Train the neural network for all desired epochs.\n",
    "    '''\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "    # Set learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.7, patience=3)\n",
    "\n",
    "    # Training loop\n",
    "    train_p_loss = []\n",
    "    val_p_loss = []\n",
    "    min_loss = 100\n",
    "    early_stopping_count = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        p_loss = train_epoch(model, train_loader, optimizer, epoch, device=device)\n",
    "        train_p_loss.append(p_loss)\n",
    "        #p_loss = test(model, device, train_loader, epoch, mode=\"train\", make_plot=True, verbose=False)\n",
    "        p_loss = test(model, val_loader, epoch, mode=\"val\")\n",
    "        val_p_loss.append(p_loss)\n",
    "        total_loss = p_loss\n",
    "        scheduler.step(total_loss)    # learning rate scheduler\n",
    "\n",
    "        if total_loss < min_loss:\n",
    "            min_loss = total_loss\n",
    "            early_stopping_count = 0\n",
    "            best_epoch = epoch\n",
    "            best_weights = deepcopy(model.state_dict())\n",
    "        else:\n",
    "            early_stopping_count += 1\n",
    "            print(f'Early Stopping Count: {early_stopping_count}')\n",
    "            if early_stopping_count == early_stopping_patience:\n",
    "                print(f\"Early Stopping. Best Epoch: {best_epoch} with loss {min_loss:.4f}.\")\n",
    "                with open(\"best_epoch.txt\", \"w\") as f:\n",
    "                    print(best_epoch, file=f)\n",
    "                break    \n",
    "\n",
    "    torch.save(best_weights, f\"model.pt\")\n",
    "    return best_weights, train_p_loss, val_p_loss\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, epoch, device=torch.device(\"cpu\")):\n",
    "    '''\n",
    "    This is your training function. When you call this function, the model is\n",
    "    trained for 1 epoch.\n",
    "    '''\n",
    "    model.train() # Set the model to training mode\n",
    "    period_losses = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()               # Clear the gradient\n",
    "        output = model(data)                # Make predictions\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()                     # Gradient computation        \n",
    "        optimizer.step()                    # Perform a single optimization step\n",
    "        period_losses.append(loss.item())\n",
    "\n",
    "        if (batch_idx*len(data)) % 10000 == 0:\n",
    "            print('Epoch: {} [{}/{} ({:3.0f}%)] Training Loss: {:9.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), period_losses[-1]), end=\" \")\n",
    "    return np.mean(period_losses)\n",
    "\n",
    "def test(model, test_loader, epoch=None, mode=None, verbose=True, device=torch.device(\"cpu\")):\n",
    "    model.eval()    # Set the model to inference mode\n",
    "    test_p_loss = 0\n",
    "    targets = []\n",
    "    preds = []\n",
    "    with torch.no_grad():   # For the inference step, gradient is not computed\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.float)\n",
    "            output = model(data)\n",
    "            targets.extend(target.cpu().numpy())\n",
    "            preds.extend(output.cpu().numpy())\n",
    "            test_p_loss += loss_function(output, target, reduction='sum').item()\n",
    "\n",
    "    test_p_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Test loss: {test_p_loss:.4f}')\n",
    "    return test_p_loss\n",
    "\n",
    "def predict(model, test_loader, verbose=True, device=torch.device(\"cpu\")):\n",
    "    model.eval()    # Set the model to inference mode\n",
    "    preds = []\n",
    "    labels = []\n",
    "    predicted = 0\n",
    "    test_p_loss = 0\n",
    "    with torch.no_grad():   # For the inference step, gradient is not computed\n",
    "        for data, target in test_loader:\n",
    "            labels.extend(np.array(target))\n",
    "            data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.float)\n",
    "            output = model(data)\n",
    "            test_p_loss += loss_function(output, target, reduction='sum').item()\n",
    "\n",
    "            preds.extend(output.cpu().numpy())\n",
    "            predicted += len(target)\n",
    "            #print(\"Progress: {}/{}\".format(predicted, len(test_loader.dataset)))\n",
    "\n",
    "    test_p_loss /= len(test_loader.dataset)\n",
    "    if verbose:\n",
    "        print(f'Test loss: {test_p_loss:.4f}')\n",
    "\n",
    "\n",
    "        return np.squeeze(preds), np.squeeze(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d5099",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [0/1000 (  0%)] Training Loss:  0.527905 Test loss: 1.0429\n",
      "Epoch: 2 [0/1000 (  0%)] Training Loss:  0.507089 Test loss: 1.0279\n",
      "Epoch: 3 [0/1000 (  0%)] Training Loss:  0.490484 Test loss: 1.0109\n",
      "Epoch: 4 [0/1000 (  0%)] Training Loss:  0.475626 Test loss: 0.9889\n",
      "Epoch: 5 [0/1000 (  0%)] Training Loss:  0.452042 Test loss: 0.9628\n",
      "Epoch: 6 [0/1000 (  0%)] Training Loss:  0.429065 Test loss: 0.9329\n",
      "Epoch: 7 [0/1000 (  0%)] Training Loss:  0.402092 Test loss: 0.8966\n",
      "Epoch: 8 [0/1000 (  0%)] Training Loss:  0.355730 Test loss: 0.8482\n",
      "Epoch: 9 [0/1000 (  0%)] Training Loss:  0.311443 Test loss: 0.7856\n",
      "Epoch: 10 [0/1000 (  0%)] Training Loss:  0.244254 Test loss: 0.7058\n",
      "Epoch: 11 [0/1000 (  0%)] Training Loss:  0.170546 Test loss: 0.6004\n",
      "Epoch: 12 [0/1000 (  0%)] Training Loss:  0.069309 Test loss: 0.4684\n",
      "Epoch: 13 [0/1000 (  0%)] Training Loss:  0.060190 Test loss: 0.3516\n",
      "Epoch: 14 [0/1000 (  0%)] Training Loss:  0.186706 Test loss: 0.2755\n",
      "Epoch: 15 [0/1000 (  0%)] Training Loss:  0.258040 Test loss: 0.2204\n",
      "Epoch: 16 [0/1000 (  0%)] Training Loss:  0.302990 Test loss: 0.1834\n",
      "Epoch: 17 [0/1000 (  0%)] Training Loss:  0.319719 Test loss: 0.1572\n",
      "Epoch: 18 [0/1000 (  0%)] Training Loss:  0.336825 Test loss: 0.1340\n",
      "Epoch: 19 [0/1000 (  0%)] Training Loss:  0.303995 Test loss: 0.1116\n",
      "Epoch: 20 [0/1000 (  0%)] Training Loss:  0.407329 Test loss: 0.0912\n",
      "Epoch: 21 [0/1000 (  0%)] Training Loss:  0.316061 Test loss: 0.0704\n",
      "Epoch: 22 [0/1000 (  0%)] Training Loss:  0.399641 Test loss: 0.0516\n",
      "Epoch: 23 [0/1000 (  0%)] Training Loss:  0.426311 Test loss: 0.0356\n",
      "Epoch: 24 [0/1000 (  0%)] Training Loss:  0.316285 Test loss: 0.0203\n",
      "Epoch: 25 [0/1000 (  0%)] Training Loss:  0.343323 Test loss: 0.0072\n",
      "Epoch: 26 [0/1000 (  0%)] Training Loss:  0.252124 Test loss: -0.0057\n",
      "Epoch: 27 [0/1000 (  0%)] Training Loss:  0.317702 Test loss: -0.0206\n",
      "Epoch: 28 [0/1000 (  0%)] Training Loss:  0.314589 Test loss: -0.0342\n",
      "Epoch: 29 [0/1000 (  0%)] Training Loss:  0.287771 Test loss: -0.0468\n",
      "Epoch: 30 [0/1000 (  0%)] Training Loss:  0.244199 Test loss: -0.0612\n",
      "Epoch: 31 [0/1000 (  0%)] Training Loss:  0.352717 Test loss: -0.0743\n",
      "Epoch: 32 [0/1000 (  0%)] Training Loss:  0.304905 Test loss: -0.0827\n",
      "Epoch: 33 [0/1000 (  0%)] Training Loss:  0.302350 Test loss: -0.0938\n",
      "Epoch: 34 [0/1000 (  0%)] Training Loss:  0.237199 Test loss: -0.1075\n",
      "Epoch: 35 [0/1000 (  0%)] Training Loss:  0.379593 Test loss: -0.1170\n",
      "Epoch: 36 [0/1000 (  0%)] Training Loss:  0.305454 Test loss: -0.1286\n",
      "Epoch: 37 [0/1000 (  0%)] Training Loss:  0.310996 Test loss: -0.1361\n",
      "Epoch: 38 [0/1000 (  0%)] Training Loss:  0.272317 Test loss: -0.1459\n",
      "Epoch: 39 [0/1000 (  0%)] Training Loss:  0.261483 Test loss: -0.1564\n",
      "Epoch: 40 [0/1000 (  0%)] Training Loss:  0.254252 Test loss: -0.1632\n",
      "Epoch: 41 [0/1000 (  0%)] Training Loss:  0.255888 Test loss: -0.1668\n",
      "Epoch: 42 [0/1000 (  0%)] Training Loss:  0.309330 Test loss: -0.1722\n",
      "Epoch: 43 [0/1000 (  0%)] Training Loss:  0.316104 Test loss: -0.1822\n",
      "Epoch: 44 [0/1000 (  0%)] Training Loss:  0.241524 Test loss: -0.1897\n",
      "Epoch: 45 [0/1000 (  0%)] Training Loss:  0.368672 Test loss: -0.1888\n",
      "Early Stopping Count: 1\n",
      "Epoch: 46 [0/1000 (  0%)] Training Loss:  0.234205 Test loss: -0.1877\n",
      "Early Stopping Count: 2\n",
      "Epoch: 47 [0/1000 (  0%)] Training Loss:  0.298320 Test loss: -0.1906\n",
      "Epoch: 48 [0/1000 (  0%)] Training Loss:  0.223986 Test loss: -0.2018\n",
      "Epoch: 49 [0/1000 (  0%)] Training Loss:  0.264296 Test loss: -0.2088\n",
      "Epoch: 50 [0/1000 (  0%)] Training Loss:  0.290515 Test loss: -0.2140\n",
      "Epoch: 51 [0/1000 (  0%)] Training Loss:  0.301477 Test loss: -0.2142\n",
      "Epoch: 52 [0/1000 (  0%)] Training Loss:  0.291279 Test loss: -0.2131\n",
      "Early Stopping Count: 1\n",
      "Epoch: 53 [0/1000 (  0%)] Training Loss:  0.306788 Test loss: -0.2139\n",
      "Early Stopping Count: 2\n",
      "Epoch: 54 [0/1000 (  0%)] Training Loss:  0.280496 Test loss: -0.2140\n",
      "Early Stopping Count: 3\n",
      "Epoch: 55 [0/1000 (  0%)] Training Loss:  0.363202 Test loss: -0.2165\n",
      "Epoch: 56 [0/1000 (  0%)] Training Loss:  0.363492 Test loss: -0.2200\n",
      "Epoch: 57 [0/1000 (  0%)] Training Loss:  0.274182 Test loss: -0.2254\n",
      "Epoch: 58 [0/1000 (  0%)] Training Loss:  0.289401 Test loss: -0.2258\n",
      "Epoch: 59 [0/1000 (  0%)] Training Loss:  0.257832 Test loss: -0.2250\n",
      "Early Stopping Count: 1\n",
      "Epoch: 60 [0/1000 (  0%)] Training Loss:  0.322640 Test loss: -0.2331\n",
      "Epoch: 61 [0/1000 (  0%)] Training Loss:  0.402400 Test loss: -0.2337\n",
      "Epoch: 62 [0/1000 (  0%)] Training Loss:  0.345642 Test loss: -0.2323\n",
      "Early Stopping Count: 1\n",
      "Epoch: 63 [0/1000 (  0%)] Training Loss:  0.230356 Test loss: -0.2352\n",
      "Epoch: 64 [0/1000 (  0%)] Training Loss:  0.499774 Test loss: -0.2376\n",
      "Epoch: 65 [0/1000 (  0%)] Training Loss:  0.277960 Test loss: -0.2403\n",
      "Epoch: 66 [0/1000 (  0%)] Training Loss:  0.242489 Test loss: -0.2437\n",
      "Epoch: 67 [0/1000 (  0%)] Training Loss:  0.417212 Test loss: -0.2408\n",
      "Early Stopping Count: 1\n",
      "Epoch: 68 [0/1000 (  0%)] Training Loss:  0.308276 Test loss: -0.2394\n",
      "Early Stopping Count: 2\n",
      "Epoch: 69 [0/1000 (  0%)] Training Loss:  0.326136 Test loss: -0.2429\n",
      "Early Stopping Count: 3\n",
      "Epoch: 70 [0/1000 (  0%)] Training Loss:  0.508359 Test loss: -0.2448\n",
      "Epoch: 71 [0/1000 (  0%)] Training Loss:  0.361757 "
     ]
    }
   ],
   "source": [
    "train_dataset = WaveletDataset(data_path=\"data\", mode=\"train\")\n",
    "valid_dataset = WaveletDataset(data_path=\"data\", mode=\"val\")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=50)\n",
    "val_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=50)\n",
    "\n",
    "model = ConvNet()\n",
    "\n",
    "weights, train_p_loss, val_p_loss = train(model, train_loader, val_loader,\n",
    "    early_stopping_patience=10, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad00a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = WaveletDataset(data_path=\"data\", mode=\"test\")\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=50)\n",
    "\n",
    "preds, trues = predict(model, test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d164df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.scatter(trues, preds[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f4376e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
