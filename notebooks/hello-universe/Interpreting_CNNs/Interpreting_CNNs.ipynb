{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf29824-b822-4311-9edd-9cba169af30d",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Interpreting Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f2448-e528-4b97-aee3-5cb3433097bb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c3c31-d679-4436-83a5-dd5a08c548f4",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "\n",
    "**In this tutorial, you will practice analyzing and interpreting a convolutional neural network.**\n",
    "This tutorial assumes a basic knowledge of convolutional neural networks. We will utilize the model described in `Classifying_JWST-HST_galaxy_mergers_with_CNNs`, so it is recommended to complete that notebook before reading this one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4bb4ff-7971-4e1a-a09e-ac7d3df222d6",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Machine learning techniques can be powerful tools for categorizing data and performing data analysis questions. However, machine learning techniques often involve a lot of hidden computation that is not immediately meaningful. The black-box nature of intermediary processes, especially in layered neural networks, can make it difficult to interpret and understand. The goal of this notebook is to familiarize you with some of the various techniques used to make sense of machine learning and convolutional neural networks (CNNs) in particular. CNNs in particular can be very difficult to interpret due to their multi-layered structure and convolutional layers. In this notebook, we will examine two methods of visualizing CNN results (Backpropagation and Grad-CAM) and another method for testing model architecture.\n",
    "\n",
    "1. Load the data\n",
    "2. Split the data into training, validation, and testing sets\n",
    "3. Build and train a model\n",
    "4. Apply some interpretation technique to understand your results from a physical perspective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ee472-cb5b-4d4c-92c4-a9c5c2209635",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "This notebook uses the following packages:\n",
    "- `numpy` to handle array functions\n",
    "- `astropy` for downloading and accessing FITS files\n",
    "- `matplotlib.pyplot` for plotting data\n",
    "- `keras` and `tensorflow` for building the CNN\n",
    "- `sklearn` for some utility functions\n",
    "\n",
    "If you do not have these packages installed, you can install them using [`pip`](https://pip.pypa.io/en/stable/) or [`conda`](https://docs.conda.io/en/latest/).\n",
    "\n",
    "Further information about the original model can be found at the [Hello Universe codebase](https://spacetelescope.github.io/hellouniverse/notebooks/hello-universe/Classifying_JWST-HST_galaxy_mergers_with_CNNs/Classifying_JWST-HST_galaxy_mergers_with_CNNs.html).\n",
    "\n",
    "\n",
    "**Author:**  \n",
    "Oliver Lin, oliverlin2004@gmail.com\n",
    "\n",
    "**Additional Contributors:**  \n",
    "Daisuke Nagai, daisuke.nagai@yale.edu.\n",
    "\n",
    "Michelle Ntampaka, mntampaka@stsci.edu.\n",
    "\n",
    "**Published:** 2024-05-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb633c50-2296-4b8d-9e8d-c805f6971ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrays\n",
    "import numpy as np\n",
    "\n",
    "# fits\n",
    "from astropy.io import fits\n",
    "from astropy.utils.data import download_file\n",
    "from astropy.visualization import simple_norm\n",
    "\n",
    "# plotting\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, BatchNormalization, Convolution2D, MaxPooling2D\n",
    "# from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tensorflow for saliency\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b9b1e-c952-4c57-9e53-438f6ce2c03d",
   "metadata": {},
   "source": [
    "### Reloading our Model\n",
    "To start, we need to reload our model from the previous galaxy classification notebook from the Mikulski Archive for Space Telescopes (MAST). The following code is directly copied over from that notebook. For a quick refresher, the model intakes a FITS file from a high level science product hosted by MAST. There are 15,426 observations in total, each taken with three filters (F814W from the Advanced Camera for Surveys and F160W from the Wide Field Camera 3 on the Hubble Space Telescope (HST), and F160W and F356W from Near Infrared Camera on the James Webb Space Telescope (JWST)). The model then applies a Convolutional Neural Network to classify whether a galaxy has undergone a merger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0403c57d-baea-40ca-9b12-ecd7a77265c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'pristine'\n",
    "file_url = 'https://archive.stsci.edu/hlsps/deepmerge/hlsp_deepmerge_hst-jwst_acs-wfc3-nircam_illustris-z2_f814w-f160w-f356w_v1_sim-'+version+'.fits'\n",
    "hdu = fits.open(download_file(file_url, cache=True, show_progress=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98010b3a-4ce5-456d-8fef-ab618e8a66ef",
   "metadata": {},
   "source": [
    "## Build and Compile the Convolutional Model\n",
    "For the sake of transparency, we will rebuild the model using the same architecture as the original notebook. The model can also be loaded directly by using `save_model` and `load_model` from the [Keras](https://www.tensorflow.org/guide/keras/serialization_and_saving) package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f5a1e-61fc-4150-9627-a4455b49580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hdu[0].data\n",
    "y = hdu[1].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df88ba0-553e-4345-8fd3-8c0026144b3c",
   "metadata": {},
   "source": [
    "Following the authors, we will split the data into 70:10:20 ratio of train:validate:test. As above, set the random seed to randomly split the images in a repeatable way. Feel free to try different values!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da768a5-c092-412b-b071-30b7d99c3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "X = np.asarray(X).astype('float32')\n",
    "y = np.asarray(y).astype('float32')\n",
    "\n",
    "# First split off 30% of the data for validation+testing\n",
    "X_train, X_split, y_train, y_split = train_test_split(X, y, test_size=0.3, random_state=random_state, shuffle=True)\n",
    "\n",
    "# Then divide this subset into training and testing sets\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_split, y_split, test_size=0.666, random_state=random_state, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4edc4f-f1f2-40ca-a26b-d26575e6faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = np.shape(X_train)[2]\n",
    "\n",
    "X_train = np.array([np.stack(x, axis=2) for x in X_train])\n",
    "X_valid = np.array([np.stack(x, axis=2) for x in X_valid])\n",
    "X_test = np.array([np.stack(x, axis=2) for x in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea42a01-8717-438b-a5de-86922e6039b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate the model architecture (written for Keras 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71731d88-faf0-48ac-914b-fe2a5f444e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define architecture for model\n",
    "data_shape = np.shape(X)\n",
    "input_shape = (imsize, imsize, 3)\n",
    "\n",
    "x_in = Input(shape=input_shape)\n",
    "c0 = Convolution2D(8, (5, 5), activation='relu', strides=(1, 1), padding='same')(x_in)\n",
    "b0 = BatchNormalization()(c0)\n",
    "d0 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')(b0)\n",
    "e0 = Dropout(0.5)(d0)\n",
    "\n",
    "c1 = Convolution2D(16, (3, 3), activation='relu', strides=(1, 1), padding='same')(e0)\n",
    "b1 = BatchNormalization()(c1)\n",
    "d1 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')(b1)\n",
    "e1 = Dropout(0.5)(d1)\n",
    "\n",
    "c2 = Convolution2D(32, (3, 3), activation='relu', strides=(1, 1), padding='same')(e1)\n",
    "b2 = BatchNormalization()(c2)\n",
    "d2 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')(b2)\n",
    "e2 = Dropout(0.5)(d2)\n",
    "\n",
    "f = Flatten()(e2)\n",
    "z0 = Dense(64, activation='softmax', kernel_regularizer=l2(0.0001))(f)\n",
    "z1 = Dense(32, activation='softmax', kernel_regularizer=l2(0.0001))(z0)\n",
    "y_out = Dense(1, activation='sigmoid')(z1)\n",
    "\n",
    "cnn = Model(inputs=x_in, outputs=y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879661d1-4e39-477e-b234-54b3e73a932c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc1556-0859-4f9d-8244-aee89658dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = 'adam'\n",
    "fit_metrics = ['accuracy']\n",
    "loss = 'binary_crossentropy'\n",
    "cnn.compile(loss=loss, optimizer=optimizer, metrics=fit_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbae334-9f09-4414-8a2c-119daef2d29f",
   "metadata": {},
   "source": [
    "### Load pretrained weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a268634-93db-4d86-82ad-5531ad3d53c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = 'https://archive.stsci.edu/hlsps/hellouniverse/hellouniverse_interpretability_best_weights.hdf5'\n",
    "cnn.load_weights(download_file(file_url, cache=True, show_progress=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c32eb-0490-4af9-bc09-07a484688d1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Saliency Maps\n",
    "We will start by introducing the most popular and well known method of interpreting CNNs. A saliency map can help us identify which pixels are significant to the models final prediction. There are many methods of calculating saliency maps, but the most popular method utilizes gradient backpropagation to determine the significance of pixels at each layer of the model. To calculate the saliency map, the error gradient at each layer is calculated and then fed into the previous layer, repeating until we reach the original image. Then the pixels with the highest gradient values will also have the most effect on the model's activation. This methodology is described in detail by <a href='https://arxiv.org/abs/1312.6034'>Simonyan et al. 2013</a>. \n",
    "\n",
    "### Dependencies\n",
    "\n",
    "`tensorflow.GradientTape()` is used to track the gradient of the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61e4c6-5401-4fbb-b97b-c0e3ba7040cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the image to analyze\n",
    "img_idx = 1\n",
    "\n",
    "# We can change the index to any number in range of the test set\n",
    "orig_img = X_test[img_idx]\n",
    "img = orig_img\n",
    "img = img.reshape((1, *img.shape))\n",
    "norm = simple_norm(orig_img, 'log', max_percent=99.75)\n",
    "scaled_img = norm(orig_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3adde0-12e9-4521-b124-e4825ce66778",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = tf.Variable(img, dtype=float)\n",
    "\n",
    "# Make a prediction and track gradients\n",
    "with tf.GradientTape() as tape:\n",
    "    pred = cnn(images, training=False)\n",
    "    class_idxs_sorted = np.argsort(pred.numpy().flatten())[::-1]    \n",
    "    loss = pred[class_idxs_sorted[0]]\n",
    "\n",
    "grads = tape.gradient(loss, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17718b97-26bb-40bb-8ade-0ea455b896a7",
   "metadata": {},
   "source": [
    "### Plot the original image and the saliency map\n",
    "\n",
    "Saliency maps provide an intuitive understanding of how the model works. The hot pixels represent higher activation and more importance. In the below model, the saliency maps demonstrates that the model focuses on the area around the center of the galaxy for the majority of galaxies. Our results are in line with a corroborating result by [Ntampaka et al. 2018](https://arxiv.org/abs/1810.07703), suggesting that the key features of a galaxy are found the ring around the galaxy rather than in the center of the galaxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daddeb53-8488-4f79-b786-8e4c706b6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cnn.predict(img)\n",
    "\n",
    "dgrad_abs = tf.math.abs(grads)\n",
    "dgrad_max_ = np.max(dgrad_abs, axis=3)[0]\n",
    "\n",
    "# normalize to range between 0 and 1\n",
    "arr_min, arr_max = np.min(dgrad_max_), np.max(dgrad_max_)\n",
    "grad_eval = (dgrad_max_ - arr_min) / (arr_max - arr_min + 1e-18)\n",
    "\n",
    "# Plot the results next to the original image\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "axes[0].imshow(orig_img)\n",
    "axes[0].set_title(\"orig_img\")\n",
    "axes[1].imshow(scaled_img)\n",
    "axes[1].set_title(\"scaled_img\")\n",
    "i = axes[2].imshow(grad_eval, cmap=\"turbo\")\n",
    "fig.colorbar(i)\n",
    "axes[2].set_title(\"heat_map\")\n",
    "fig.suptitle(\"prediction_val=\" + str(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061c24d-efdd-4107-bb07-f9d29d03bcec",
   "metadata": {},
   "source": [
    "**Image caption**: The above image contains three panels in a horizontal row. The first panel shows the original image of a merging galaxy candidate, the second panel shows a logarithmically-scaled version of the original image, and the third panel shows a heat map of the saliency map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241cba9",
   "metadata": {},
   "source": [
    "### Create a stack of saliency images\n",
    "\n",
    "To understand the overall behavior of our algorithm, we can stack some or all of the saliency maps in the test set to generate an overarching estimate of important pixels. For the sake of simplicity, we will stack the saliency maps for the first 100 images in the test set. Our results once again indicate that the region around a galaxy is particularly important to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06af4464-c432-4536-a876-6b1202f7d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_map = np.zeros((75, 75))\n",
    "# Summing the first 100 saliencies. We can change\n",
    "# the range to sum more or less saliencies or pick \n",
    "# specific ones\n",
    "for i in range(100):\n",
    "    img = X_test[img_idx]\n",
    "    img = img.reshape((1, *img.shape))\n",
    "    images = tf.Variable(img, dtype=float)\n",
    "\n",
    "    # Make a prediction and track gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = cnn(images, training=False)\n",
    "        class_idxs_sorted = np.argsort(pred.numpy().flatten())[::-1]    \n",
    "        loss = pred[class_idxs_sorted[0]]\n",
    "\n",
    "    grads = tape.gradient(loss, images)\n",
    "\n",
    "    y_pred = cnn.predict(images, verbose=0)\n",
    "\n",
    "    dgrad_abs = tf.math.abs(grads)\n",
    "    dgrad_max_ = np.max(dgrad_abs, axis=3)[0]\n",
    "\n",
    "    # normalize to range between 0 and 1\n",
    "    arr_min, arr_max = np.min(dgrad_max_), np.max(dgrad_max_)\n",
    "    grad_eval = (dgrad_max_ - arr_min) / (arr_max - arr_min)\n",
    "    sum_map += grad_eval\n",
    "plt.imshow(sum_map, cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd4430-0ca1-42cb-994f-6f66b41e0953",
   "metadata": {},
   "source": [
    "**Image caption**: The above image contains a single panel, and shows a stacked version of the saliency maps from 100 images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a521f3-f476-4b5e-b46d-2cba675ae992",
   "metadata": {},
   "source": [
    "## 2. Grad-CAM\n",
    "While gradient backpropagation has historically been the most popular type of saliency map, the highly connected nature of backtracking has been shown to produce high variance under small changes to inputs. As such, gradient backpropagation is extremely sensitive to data manipulation (preprocessing, sensitivity analysis, GANs), raising questions about its reliability and validity. Gradient Class Activation Mapping (Grad-CAM) is an alternative method for generating saliency models that only examines the gradient of the final convolutional layer when producing the map. As a consequence, Grad-CAM maps have lower (coarser) resolution than backpropagation but are far more resilient to small changes and therefore more reliable when tuning a model. A full description of the technique can be found in <a href='https://arxiv.org/abs/1610.02391'>Selveraju et al. 2016</a>.\n",
    "\n",
    "The code for Grad-CAM comes from a useful tutorial on the subject by Daniel Reiff. For more information, please visit the  <a href='https://towardsdatascience.com/understand-your-algorithm-with-grad-cam-d3b62fce353'>full tutorial</a>.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "`Open_CV` and `astropy.simple_norm` are used to do manipulate the image for display. Alternatively, we could allow Python to automatically clip the image when the heatmap is out of range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b67615-dda4-4f3a-82f1-236510e62270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the image to analyze\n",
    "img_idx = 1\n",
    "\n",
    "# We can change the index to any number in range of the test set\n",
    "orig_img = X_test[img_idx]\n",
    "img = orig_img\n",
    "img = img.reshape((1, *img.shape))\n",
    "norm = simple_norm(orig_img, 'log', max_percent=99.75)\n",
    "scaled_img = norm(orig_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88fceb4-f3e1-43d7-a2c2-b04ea1725117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: recompiling the model will change the layer\n",
    "# name. In that case, you can either restart the \n",
    "# kernel or change the layer_name.\n",
    "# We can also change the layer selected here to pull out any layer of our model\n",
    "gradModel = Model(inputs=[cnn.inputs], outputs=[cnn.get_layer(\"conv2d_2\").output, cnn.output])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # get the loss with associated with the prediction\n",
    "    inputs = tf.cast(X_test, tf.float32)\n",
    "    (convOutputs, predictions) = gradModel(inputs)\n",
    "    loss = predictions[:, 0]\n",
    "    \n",
    "# use automatic differentiation to compute the gradients\n",
    "grads = tape.gradient(loss, convOutputs)\n",
    "\n",
    "# compute the guided gradients by removing all nonpositive\n",
    "# gradients\n",
    "castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
    "castGrads = tf.cast(grads > 0, \"float32\")\n",
    "guidedGrads = castConvOutputs * castGrads * grads\n",
    "\n",
    "# pick out the convolution and gradient of the chosen image\n",
    "convOutputs = convOutputs[img_idx]\n",
    "guidedGrads = guidedGrads[img_idx]\n",
    "\n",
    "# compute the average of the gradient values, and using them\n",
    "# as weights, compute the importance of the pieces\n",
    "weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
    "cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
    "\n",
    "# grab the spatial dimensions of the input image and resize\n",
    "# the output class activation map to match the input image\n",
    "# dimensions\n",
    "(w, h) = (X_test.shape[2], X_test.shape[1])\n",
    "heatmap = cv2.resize(cam.numpy(), (w, h))\n",
    "\n",
    "# normalize the heatmap such that all values lie in the range\n",
    "# [0, 1], scale the resulting values to the range [0, 255],\n",
    "# and then convert to an unsigned 8-bit integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad137fa-2d40-48f6-b2c4-1dc50b106eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cnn.predict(img)\n",
    "\n",
    "# Plot the results next to the original image\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "axes[0].imshow(orig_img)\n",
    "axes[0].set_title(\"orig_img\")\n",
    "axes[1].imshow(scaled_img)\n",
    "axes[1].set_title(\"scaled_img\")\n",
    "i = axes[2].imshow(heatmap, cmap=\"turbo\")\n",
    "fig.colorbar(i)\n",
    "axes[2].set_title(\"heat_map\")\n",
    "fig.suptitle(\"prediction_val=\" + str(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8992e02-d1d2-4314-8849-e4db7932469e",
   "metadata": {},
   "source": [
    "**Image caption**: The above image contains three panels in a horizontal row. The first panel shows the original image of a merging galaxy candidate, the second panel shows a logarithmically-scaled version of the original image, and the third panel shows a heat map of the grad-cam map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a59ff-53b3-4353-804c-5061281d5b3a",
   "metadata": {},
   "source": [
    "**The result is very similar** to our saliency map from backpropagation, although the image is coarser and also shows both the top and bottom of the galaxy. We can also play around with the selected layer to calculate the output at different steps in the model and see how activation changes throughout the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27909148-c421-48b6-9ecb-a47f56a5dd61",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. RISE Algorithm\n",
    "The RISE (Randomized Input Sampling for Explanation) Algorithm is another interpretation technique for calculating saliency maps. Instead of calculating gradients from within the model, the RISE implementation works by covering up pieces of the input image, running it through the model, and calculating the average activation in order to determine what parts of the image are most important. As such, this method does not require any access to the inner workings of the model. The algorithm first generates a random sequence of binary grids (called masks), which are placed onto the image. Everything not covered by the mask is removed by multiplying the images together, and the resultant activations are averaged to get our final heatmap. A full description of the algorithm and its variations is provided by <a href='https://arxiv.org/abs/1806.07421'>Petsiuk et al. 2018</a>. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6c0619-5f75-4f8f-b3c4-2611f987672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the image to analyze\n",
    "img_idx = 6\n",
    "\n",
    "# We can change the index to any number in range of the test set\n",
    "image = X_test[img_idx]\n",
    "\n",
    "N = 1000  # Number of masks\n",
    "s = 8     # Size of the grid\n",
    "p1 = 0.5  # Probability of the cell being set to 1\n",
    "\n",
    "cell_size = np.ceil(np.array(input_shape[:2]) / s).astype(int)\n",
    "up_size = (s * cell_size).astype(int)\n",
    "\n",
    "grid = np.random.rand(N, s, s) < p1\n",
    "masks = np.empty((N, *input_shape[:2]))\n",
    "\n",
    "for i in range(N):\n",
    "    # Randomly place the grid on the image\n",
    "    x = np.random.randint(0, input_shape[0]-s)\n",
    "    y = np.random.randint(0, input_shape[1]-s)\n",
    "    mask = np.pad(grid[i], ((x, input_shape[0]-x-s), (y, input_shape[0]-y-s)), 'constant', constant_values=(0, 0))\n",
    "    mask = mask[:input_shape[0], :input_shape[1]]\n",
    "    masks[i] = mask\n",
    "\n",
    "masks = masks.reshape(-1, *input_shape[:2], 1)\n",
    "\n",
    "N = len(masks)\n",
    "pred_masks = cnn.predict(image * masks)\n",
    "pred_masks = np.expand_dims(pred_masks, axis=-1)\n",
    "pred_masks = np.expand_dims(pred_masks, axis=-1) # Reshape pred_masks for broadcasting\n",
    "heatmap = (pred_masks * masks).sum(axis=0)\n",
    "heatmap = heatmap / N / p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a6e994-1360-45d8-a6a4-0f8e3bc9bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results next to the original image\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title(\"orig_img\")\n",
    "i = axes[1].imshow(heatmap, cmap=\"turbo\")\n",
    "fig.colorbar(i)\n",
    "axes[1].set_title(\"heat_map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfc889d-efc0-4275-918f-26c86d7d8402",
   "metadata": {},
   "source": [
    "**Image caption**: The above image contains two panels in a horizontal row. The first panel shows the original image of a merginig galaxy candidate, and the second panel shows a heat map of the RISE map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bcaa55-1884-4665-8040-61e16ab35c90",
   "metadata": {},
   "source": [
    "**When examining the selected image**, we see a ring around the galactic center. Note that this is not the case for all astronomical images, or even all images in this dataset. Try playing around with the selected image to generate different saliency maps. For images where the central feature is relatively small, RISE's occlusion-based methodology can be unreliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdc67cb-5a3d-4504-a7be-85992a535154",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Ablation Analysis\n",
    "Saliency maps provide an intuitive visual understanding of our model's focus and can be useful for understanding the physical relevance of our classification scheme. In order to understand the efficacy of our model's architecture, we can instead use ablation analysis to determine the most important layers of our model. Ablation analysis works by rebuilding our model without a specified layer of interest and testing and comparing the performance of a partial model. Since we are focusing on our model's internal architecture rather than the features of the dataset we are looking at, we want to use this technique when trying to improve the training metrics of our model by editing its layers. This method allows us to determine which layers of the model are most important, or if some layers are hindering the learning capabilities of our mode\n",
    "\n",
    "In the exercise below, we will build and train four mini-models on the same data set as before. As this is an educational notebook, we will limit the training time of each model to five epochs. Results with these models may vary considerably due to these training constraints, but we highly encourage you to try modifying this section of the notebook for different results (see [Exercises](#Exercises/Extensions))\n",
    "\n",
    "Note that performing an ablation analysis will require training multiple models with the same architecture. This can be quite compute intensive on personal computers, so if you are running this notebook locally it is recommended that your device be plugged in before running the analysis.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "`Tensorflow` is used build our model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bda985c-0a32-418e-a1f7-b0c428ea0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(ablate=None):\n",
    "    x_in = Input(shape=input_shape)\n",
    "    \n",
    "    if ablate != 'c0':\n",
    "        c0 = Convolution2D(8, (5, 5), activation='relu', strides=(1, 1), padding='same')(x_in)\n",
    "    else:\n",
    "        c0 = x_in\n",
    "    b0 = BatchNormalization()(c0)\n",
    "    d0 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')(b0)\n",
    "    e0 = Dropout(0.5)(d0)\n",
    "\n",
    "    if ablate != 'c1':\n",
    "        c1 = Convolution2D(16, (3, 3), activation='relu', strides=(1, 1), padding='same')(e0)\n",
    "    else:\n",
    "        c1 = e0\n",
    "    b1 = BatchNormalization()(c1)\n",
    "    d1 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')(b1)\n",
    "    e1 = Dropout(0.5)(d1)\n",
    "\n",
    "    if ablate != 'c2':\n",
    "        c2 = Convolution2D(32, (3, 3), activation='relu', strides=(1, 1), padding='same')(e1)\n",
    "    else:\n",
    "        c2 = e1\n",
    "    b2 = BatchNormalization()(c2)\n",
    "    d2 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')(b2)\n",
    "    e2 = Dropout(0.5)(d2)\n",
    "\n",
    "    f = Flatten()(e2)\n",
    "    z0 = Dense(64, activation='softmax', kernel_regularizer=l2(0.0001))(f)\n",
    "    z1 = Dense(32, activation='softmax', kernel_regularizer=l2(0.0001))(z0)\n",
    "    y_out = Dense(1, activation='sigmoid')(z1)\n",
    "\n",
    "    cnn = Model(inputs=x_in, outputs=y_out)\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d419c4cc-ad09-49db-b903-636eec3577d0",
   "metadata": {},
   "source": [
    "**Since ablation analysis requires training multiple models**, it can often be more resource intensive than other methods. However, it can also provide useful information on the way features are organized during training. The following cell can be edited to change how much we want to train our mini-models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef61819e-4804-4c17-be05-31255e0522da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can change how much to train each model\n",
    "# 5 epochs is chosen due to time and computation constraints\n",
    "num_epochs = 5\n",
    "\n",
    "# Train the original model\n",
    "model = create_model()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=num_epochs, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Get baseline performance\n",
    "baseline_score = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Ablate each layer and compare performance\n",
    "layers_to_ablate = ['c0', 'c1', 'c2']\n",
    "for layer in layers_to_ablate:\n",
    "    model_ablated = create_model(ablate=layer)\n",
    "    model_ablated.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model_ablated.fit(X_train, y_train, epochs=num_epochs, batch_size=32, validation_data=(X_test, y_test))\n",
    "    ablated_score = model_ablated.evaluate(X_test, y_test)\n",
    "    \n",
    "    print(f\"Performance drop after ablating {layer}: {baseline_score[1] - ablated_score[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0a2ddf-665a-4ddc-a69b-417f149253b6",
   "metadata": {},
   "source": [
    "## FAQ\n",
    "\n",
    "- **How do I choose which saliency model to use?** Each saliency technique has it's own strengths and weaknesses. If you are looking for an intuitive approach that produces the highest resolution image, then gradient backpropagation is the best choice. If you want to control which layer to look at and watch how the model evolves or are concerned about reliability, then Grad-CAM will perform better. If you don't have access to the inner workings of the model or want to understand how a model would perform with slightly different inputs, then RISE is the best option. As a reminder, ablation analysis is used when trying to improve the training metrics of our model by editing its layers. This method allows us to determine which layers of the model are most important, or if some layers are hindering the learning capabilities of our model.\n",
    "\n",
    "\n",
    "- **Why does my ablation analysis have a negative performance drop?** A negative performance drop suggests that removing a layer would be beneficial to the accuracy of the model, which can actually be the case. In this particular scenario, however, it is more likely that your model ran into a fluke while training. Try cranking up the number of epochs and retraining for longer periods of time to get more consistent results.\n",
    "\n",
    "\n",
    "- **What kind of models can these approaches be applied to?** Many of these interpretation methods were chosen to be intentionally broad and general. Ablation analysis is a common technique used to test pretty much any deep neural network. While saliency maps can only be generated for image objects, we can still apply backpropagation and random input sampling on other models to pull out important features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1694630-29d6-43c3-8c09-95ed1a856379",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercises/Extensions\n",
    "\n",
    "**Viewing different layers.** Go back to the Grad-CAM implementation and select the first convolutional layer instead (typically named \"conv2d\"). Run the code and compare your heatmap with the one produced by the third layer. What differences do you notice in terms of resolution and regional focus? Repeat with the second convolutional layer. How does the model's focus change?\n",
    "\n",
    "**Changing kernel size.** Go back to the RISE implementation and try playing around with the size of the mask. How does this impact the saliency image generated? Try playing around with N (the number of masks) and p1 (the probability of a given cell being visible). How does the image change as N increases? 0.5 is a commonly accepted value of p1. What problems do we run into at small and large p1?\n",
    "\n",
    "**Other RISE implementations.** The RISE algorithm can be implemented many different ones. In the given example, we create patches of visibility on an otherwise covered image. Try reversing the algorithm so that masks black out a piece of the image and leave the rest visible. How does this affect our saliency map? Which of these two methods is better in this particular case and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72635b5c-1c38-44ae-913b-bced3619d58b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## About this Notebook\n",
    "\n",
    "**Info:**  \n",
    "This notebook is based on the code repository for the paper <a href=\"https://doi.org/10.1016/j.ascom.2020.100390\">\"DeepMerge: Classifying High-redshift Merging Galaxies with Deep Neural Networks\"</a>, A. Ćiprijanović, G.F. Snyder, B. Nord, J.E.G. Peek, Astronomy & Computing, Volume 32, July 2020, and the notebook \"CNN_for_cluster_masses\" by Michelle Ntampaka, Assistant Astronomer, mntampaka@stsci.edu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82ee7c7-1199-4385-bea1-0d9a1d4ff16d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Citations\n",
    "\n",
    "If you use this data set, `astropy`, or `keras` for published research, please cite the\n",
    "authors. Follow these links for more information:\n",
    "\n",
    "* [Citing the data set](https://www.sciencedirect.com/science/article/pii/S2213133720300445#fn3)\n",
    "* [Citing `astropy`](https://www.astropy.org/acknowledging.html)\n",
    "* [Citing `keras`](https://keras.io/getting_started/faq/#how-should-i-cite-keras)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
